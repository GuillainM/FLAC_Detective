name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
  schedule:
    # Run weekly benchmarks every Sunday at 3 AM
    - cron: '0 3 * * 0'

permissions:
  contents: write
  deployments: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libsndfile1 flac

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark pytest-benchmark[histogram]

      - name: Create benchmark directory
        run: |
          mkdir -p benchmark-results

      - name: Run benchmarks
        continue-on-error: true
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results/output.json \
            --benchmark-min-rounds=5 \
            --benchmark-warmup=on \
            --benchmark-autosave \
            -v

      - name: Generate benchmark report
        continue-on-error: true
        run: |
          python scripts/generate_benchmark_report.py \
            benchmark-results/output.json \
            > benchmark-results/report.md

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          name: FLAC Detective Performance
          tool: 'pytest'
          output-file-path: benchmark-results/output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Show alert with commit comment on detecting possible performance regression
          alert-threshold: '130%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@GuillainM'
          # Store results in gh-pages branch
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results/
          retention-days: 90

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            // Read benchmark report
            let report = '';
            try {
              report = fs.readFileSync('benchmark-results/report.md', 'utf8');
            } catch (error) {
              report = '‚ö†Ô∏è Could not read benchmark report';
            }

            // Post comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `## üìä Performance Benchmark Results\n\n${report}\n\n` +
                    `<details>\n<summary>View detailed results</summary>\n\n` +
                    `Download the benchmark artifacts for detailed analysis.\n` +
                    `</details>`
            });

  compare-benchmarks:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark

      - name: Download current benchmark
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: current-results/

      - name: Checkout main branch
        run: |
          git fetch origin main:main

      - name: Run baseline benchmarks (main branch)
        run: |
          git checkout main
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=baseline-results/output.json \
            --benchmark-min-rounds=3 \
            -v || echo "Baseline benchmarks not available"
          git checkout -

      - name: Compare benchmarks
        run: |
          python scripts/compare_benchmarks.py \
            baseline-results/output.json \
            current-results/output.json \
            > comparison.md || echo "## No baseline available for comparison"

      - name: Comment comparison
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let comparison = '';
            try {
              comparison = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comparison = '‚ö†Ô∏è Could not generate benchmark comparison';
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `## üìà Benchmark Comparison (PR vs Main)\n\n${comparison}`
            });

  performance-regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: results/

      - name: Check for performance regressions
        run: |
          python scripts/check_performance_regression.py \
            results/output.json \
            --threshold 30 \
            --output regression-report.md

      - name: Upload regression report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: regression-report
          path: regression-report.md
          retention-days: 30

      - name: Fail on severe regression
        run: |
          if [ -f regression-report.md ]; then
            if grep -q "SEVERE REGRESSION" regression-report.md; then
              echo "::error::Severe performance regression detected!"
              cat regression-report.md
              exit 1
            fi
          fi

  summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmark, compare-benchmarks, performance-regression-check]
    if: always()

    steps:
      - name: Create workflow summary
        run: |
          echo "## üìä Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.benchmark.result }}" == "success" ]; then
            echo "‚úÖ **Benchmarks**: Completed successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Benchmarks**: Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.compare-benchmarks.result }}" == "success" ]; then
            echo "‚úÖ **Comparison**: Completed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ github.event_name }}" != "pull_request" ]; then
            echo "‚è≠Ô∏è **Comparison**: Skipped (not a PR)" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Comparison**: Failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-regression-check.result }}" == "success" ]; then
            echo "‚úÖ **Regression Check**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è **Regression Check**: Issues detected" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìà View Results" >> $GITHUB_STEP_SUMMARY
          echo "- [Benchmark History](https://github.com/${{ github.repository }}/blob/gh-pages/dev/bench/index.html)" >> $GITHUB_STEP_SUMMARY
          echo "- Download artifacts for detailed analysis" >> $GITHUB_STEP_SUMMARY
